{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "from torch.autograd import Function\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "from sklearn import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# from torchsummary import summary\n",
    "\n",
    "import qiskit\n",
    "from qiskit.visualization import *\n",
    "from qiskit.circuit.random import random_circuit\n",
    "\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f0d648d0690>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 256\n",
    "EPOCHS = 10     # Number of optimization epochs\n",
    "n_layers = 1    # Number of random layers\n",
    "n_train = 50    # Size of the train dataset\n",
    "n_test = 30     # Size of the test dataset\n",
    "\n",
    "SAVE_PATH = \"quanvolution/\" # Data saving folder\n",
    "PREPROCESS = True           # If False, skip quantum processing and load data from SAVE_PATH\n",
    "seed = 47\n",
    "np.random.seed(seed)        # Seed for NumPy random number generator\n",
    "torch.manual_seed(seed)     # Seed for TensorFlow random number generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('./data/', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.Resize(14),\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('./data/', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.Resize(14),\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the quantum circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuanvCircuit:\n",
    "    \"\"\" \n",
    "    This class defines filter circuit of Quanvolution layer\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, kernel_size, backend, shots, threshold):\n",
    "        # --- Circuit definition start ---\n",
    "        self.n_qubits = kernel_size ** 2\n",
    "        self._circuit = qiskit.QuantumCircuit(self.n_qubits)\n",
    "        self.theta = [qiskit.circuit.Parameter('theta{}'.format(i)) for i in range(self.n_qubits)]\n",
    "\n",
    "        for i in range(self.n_qubits):\n",
    "            self._circuit.rx(self.theta[i], i)\n",
    "        \n",
    "        self._circuit.barrier()\n",
    "        self._circuit += random_circuit(self.n_qubits, 2)\n",
    "        self._circuit.measure_all()\n",
    "        # ---- Circuit definition end ----\n",
    "\n",
    "        self.backend   = backend\n",
    "        self.shots     = shots\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def run(self, data):\n",
    "        # data shape: tensor (1, 5, 5)\n",
    "        # val > self.threshold  : |1> - rx(pi)\n",
    "        # val <= self.threshold : |0> - rx(0)\n",
    "\n",
    "        # reshape input data\n",
    "        # [1, kernel_size, kernel_size] -> [1, self.n_qubits]\n",
    "        data = torch.reshape(data, (1, self.n_qubits))\n",
    "\n",
    "        # encoding data to parameters\n",
    "        thetas = []\n",
    "        for dat in data:\n",
    "            theta = []\n",
    "            for val in dat:\n",
    "                if val > self.threshold:\n",
    "                    theta.append(np.pi)\n",
    "                else:\n",
    "                    theta.append(0)\n",
    "            thetas.append(theta)\n",
    "        \n",
    "        param_dict = dict()\n",
    "        for theta in thetas:\n",
    "            for i in range(self.n_qubits):\n",
    "                param_dict[self.theta[i]] = theta[i]\n",
    "        param_binds = [param_dict]\n",
    "\n",
    "        # execute random quantum circuit\n",
    "        job = qiskit.execute(self._circuit, \n",
    "                             self.backend, \n",
    "                             shots = self.shots, \n",
    "                             parameter_binds = param_binds)\n",
    "        result = job.result().get_counts(self._circuit)\n",
    "\n",
    "        # decoding the result\n",
    "        counts = 0\n",
    "        for key, val in result.items():\n",
    "            cnt = sum([int(char) for char in key])\n",
    "            counts += cnt * val\n",
    "\n",
    "        # Compute probabilities for each state\n",
    "        probabilities = counts / (self.shots * self.n_qubits)\n",
    "        # probabilities = counts / self.shots\n",
    "        \n",
    "        return probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Quanvolution Class with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuanvFunction(Function):\n",
    "    \"\"\" Quanv function definition \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, inputs, in_channels, out_channels, kernel_size, quantum_circuits, shift):\n",
    "        #forward pass of the quanvolutional function\n",
    "        return 0\n",
    "        \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        #backwards pass of the quanvolutional function\n",
    "        return 0\n",
    "\n",
    "\n",
    "class Quanv(nn.Module):\n",
    "    \"\"\" Quanvolution(Quantum convolution) layer definition \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, kernel_size, \n",
    "                 backend=qiskit.Aer.get_backend('qasm_simulator'), \n",
    "                 shots=100, shift=np.pi/2):\n",
    "        super(Quanv, self).__init__()\n",
    "        self.quantum_circuits = [QuanvCircuit(kernel_size=kernel_size, \n",
    "                                              backend=backend, shots=shots, threshold=127) \n",
    "                                 for i in range(out_channels)]\n",
    "        self.in_channels  = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size  = kernel_size\n",
    "        self.shift        = shift\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        return QuanvFunction.apply(inputs, self.in_channels, self.out_channels, self.kernel_size,\n",
    "                                   self.quantum_circuits, self.shift)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # self.quanv = Quanv(1, 6, kernel_size=5)\n",
    "        self.conv = nn.Conv2d(1, 16, kernel_size=5)\n",
    "        # out_channels = 16, kernel size = 5, next input is 400 (16*5*5)\n",
    "        # out_channels = 16, kernel size = 3, next input is 576 (64*3*3)\n",
    "        self.dropout = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(400, 64)\n",
    "        self.fc2 = nn.Linear(64, 10) #10 possible classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = F.relu(self.quanv(x))\n",
    "        # x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.conv(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_704/4258743190.py:21: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training [5%]\tLoss: 1.7550\n",
      "Training [10%]\tLoss: 1.5775\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/4ccha/OneDrive - Queen's University/Design Teams/QMIND/qmindgithub/QMIND-VQC/src/NeuralNetworkObjects/torchClass.ipynb Cell 14'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/4ccha/OneDrive%20-%20Queen%27s%20University/Design%20Teams/QMIND/qmindgithub/QMIND-VQC/src/NeuralNetworkObjects/torchClass.ipynb#ch0000013vscode-remote?line=8'>9</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/4ccha/OneDrive%20-%20Queen%27s%20University/Design%20Teams/QMIND/qmindgithub/QMIND-VQC/src/NeuralNetworkObjects/torchClass.ipynb#ch0000013vscode-remote?line=9'>10</a>\u001b[0m     total_loss \u001b[39m=\u001b[39m []\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/4ccha/OneDrive%20-%20Queen%27s%20University/Design%20Teams/QMIND/qmindgithub/QMIND-VQC/src/NeuralNetworkObjects/torchClass.ipynb#ch0000013vscode-remote?line=10'>11</a>\u001b[0m     \u001b[39mfor\u001b[39;00m batch_idx, (data, target) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/4ccha/OneDrive%20-%20Queen%27s%20University/Design%20Teams/QMIND/qmindgithub/QMIND-VQC/src/NeuralNetworkObjects/torchClass.ipynb#ch0000013vscode-remote?line=11'>12</a>\u001b[0m         optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/4ccha/OneDrive%20-%20Queen%27s%20University/Design%20Teams/QMIND/qmindgithub/QMIND-VQC/src/NeuralNetworkObjects/torchClass.ipynb#ch0000013vscode-remote?line=13'>14</a>\u001b[0m         \u001b[39m# Forward pass\u001b[39;00m\n",
      "File \u001b[0;32m~/.virtualenvs/qmind/lib/python3.8/site-packages/torch/utils/data/dataloader.py:521\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/connorc84/.virtualenvs/qmind/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=518'>519</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/connorc84/.virtualenvs/qmind/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=519'>520</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[0;32m--> <a href='file:///home/connorc84/.virtualenvs/qmind/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=520'>521</a>\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    <a href='file:///home/connorc84/.virtualenvs/qmind/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=521'>522</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    <a href='file:///home/connorc84/.virtualenvs/qmind/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=522'>523</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    <a href='file:///home/connorc84/.virtualenvs/qmind/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=523'>524</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    <a href='file:///home/connorc84/.virtualenvs/qmind/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=524'>525</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.virtualenvs/qmind/lib/python3.8/site-packages/torch/utils/data/dataloader.py:561\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/connorc84/.virtualenvs/qmind/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=558'>559</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///home/connorc84/.virtualenvs/qmind/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=559'>560</a>\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/connorc84/.virtualenvs/qmind/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=560'>561</a>\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/connorc84/.virtualenvs/qmind/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=561'>562</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    <a href='file:///home/connorc84/.virtualenvs/qmind/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=562'>563</a>\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data)\n",
      "File \u001b[0;32m~/.virtualenvs/qmind/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     <a href='file:///home/connorc84/.virtualenvs/qmind/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py?line=46'>47</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     <a href='file:///home/connorc84/.virtualenvs/qmind/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py?line=47'>48</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> <a href='file:///home/connorc84/.virtualenvs/qmind/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py?line=48'>49</a>\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     <a href='file:///home/connorc84/.virtualenvs/qmind/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py?line=49'>50</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='file:///home/connorc84/.virtualenvs/qmind/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py?line=50'>51</a>\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.virtualenvs/qmind/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     <a href='file:///home/connorc84/.virtualenvs/qmind/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py?line=46'>47</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     <a href='file:///home/connorc84/.virtualenvs/qmind/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py?line=47'>48</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> <a href='file:///home/connorc84/.virtualenvs/qmind/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py?line=48'>49</a>\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     <a href='file:///home/connorc84/.virtualenvs/qmind/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py?line=49'>50</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='file:///home/connorc84/.virtualenvs/qmind/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py?line=50'>51</a>\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.virtualenvs/qmind/lib/python3.8/site-packages/torchvision/datasets/mnist.py:131\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    <a href='file:///home/connorc84/.virtualenvs/qmind/lib/python3.8/site-packages/torchvision/datasets/mnist.py?line=126'>127</a>\u001b[0m img, target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata[index], \u001b[39mint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtargets[index])\n\u001b[1;32m    <a href='file:///home/connorc84/.virtualenvs/qmind/lib/python3.8/site-packages/torchvision/datasets/mnist.py?line=128'>129</a>\u001b[0m \u001b[39m# doing this so that it is consistent with all other datasets\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/connorc84/.virtualenvs/qmind/lib/python3.8/site-packages/torchvision/datasets/mnist.py?line=129'>130</a>\u001b[0m \u001b[39m# to return a PIL Image\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/connorc84/.virtualenvs/qmind/lib/python3.8/site-packages/torchvision/datasets/mnist.py?line=130'>131</a>\u001b[0m img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39;49mfromarray(img\u001b[39m.\u001b[39;49mnumpy(), mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mL\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m    <a href='file:///home/connorc84/.virtualenvs/qmind/lib/python3.8/site-packages/torchvision/datasets/mnist.py?line=132'>133</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/connorc84/.virtualenvs/qmind/lib/python3.8/site-packages/torchvision/datasets/mnist.py?line=133'>134</a>\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform(img)\n",
      "File \u001b[0;32m~/.virtualenvs/qmind/lib/python3.8/site-packages/PIL/Image.py:2834\u001b[0m, in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   <a href='file:///home/connorc84/.virtualenvs/qmind/lib/python3.8/site-packages/PIL/Image.py?line=2830'>2831</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   <a href='file:///home/connorc84/.virtualenvs/qmind/lib/python3.8/site-packages/PIL/Image.py?line=2831'>2832</a>\u001b[0m         obj \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mtostring()\n\u001b[0;32m-> <a href='file:///home/connorc84/.virtualenvs/qmind/lib/python3.8/site-packages/PIL/Image.py?line=2833'>2834</a>\u001b[0m \u001b[39mreturn\u001b[39;00m frombuffer(mode, size, obj, \u001b[39m\"\u001b[39;49m\u001b[39mraw\u001b[39;49m\u001b[39m\"\u001b[39;49m, rawmode, \u001b[39m0\u001b[39;49m, \u001b[39m1\u001b[39;49m)\n",
      "File \u001b[0;32m~/.virtualenvs/qmind/lib/python3.8/site-packages/PIL/Image.py:2756\u001b[0m, in \u001b[0;36mfrombuffer\u001b[0;34m(mode, size, data, decoder_name, *args)\u001b[0m\n\u001b[1;32m   <a href='file:///home/connorc84/.virtualenvs/qmind/lib/python3.8/site-packages/PIL/Image.py?line=2753'>2754</a>\u001b[0m     args \u001b[39m=\u001b[39m mode, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m\n\u001b[1;32m   <a href='file:///home/connorc84/.virtualenvs/qmind/lib/python3.8/site-packages/PIL/Image.py?line=2754'>2755</a>\u001b[0m \u001b[39mif\u001b[39;00m args[\u001b[39m0\u001b[39m] \u001b[39min\u001b[39;00m _MAPMODES:\n\u001b[0;32m-> <a href='file:///home/connorc84/.virtualenvs/qmind/lib/python3.8/site-packages/PIL/Image.py?line=2755'>2756</a>\u001b[0m     im \u001b[39m=\u001b[39m new(mode, (\u001b[39m1\u001b[39;49m, \u001b[39m1\u001b[39;49m))\n\u001b[1;32m   <a href='file:///home/connorc84/.virtualenvs/qmind/lib/python3.8/site-packages/PIL/Image.py?line=2756'>2757</a>\u001b[0m     im \u001b[39m=\u001b[39m im\u001b[39m.\u001b[39m_new(core\u001b[39m.\u001b[39mmap_buffer(data, size, decoder_name, \u001b[39m0\u001b[39m, args))\n\u001b[1;32m   <a href='file:///home/connorc84/.virtualenvs/qmind/lib/python3.8/site-packages/PIL/Image.py?line=2757'>2758</a>\u001b[0m     im\u001b[39m.\u001b[39mreadonly \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/.virtualenvs/qmind/lib/python3.8/site-packages/PIL/Image.py:2656\u001b[0m, in \u001b[0;36mnew\u001b[0;34m(mode, size, color)\u001b[0m\n\u001b[1;32m   <a href='file:///home/connorc84/.virtualenvs/qmind/lib/python3.8/site-packages/PIL/Image.py?line=2651'>2652</a>\u001b[0m \u001b[39mif\u001b[39;00m color \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   <a href='file:///home/connorc84/.virtualenvs/qmind/lib/python3.8/site-packages/PIL/Image.py?line=2652'>2653</a>\u001b[0m     \u001b[39m# don't initialize\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/connorc84/.virtualenvs/qmind/lib/python3.8/site-packages/PIL/Image.py?line=2653'>2654</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m Image()\u001b[39m.\u001b[39m_new(core\u001b[39m.\u001b[39mnew(mode, size))\n\u001b[0;32m-> <a href='file:///home/connorc84/.virtualenvs/qmind/lib/python3.8/site-packages/PIL/Image.py?line=2655'>2656</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39;49m(color, \u001b[39mstr\u001b[39m):\n\u001b[1;32m   <a href='file:///home/connorc84/.virtualenvs/qmind/lib/python3.8/site-packages/PIL/Image.py?line=2656'>2657</a>\u001b[0m     \u001b[39m# css3-style specifier\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/connorc84/.virtualenvs/qmind/lib/python3.8/site-packages/PIL/Image.py?line=2658'>2659</a>\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m ImageColor\n\u001b[1;32m   <a href='file:///home/connorc84/.virtualenvs/qmind/lib/python3.8/site-packages/PIL/Image.py?line=2660'>2661</a>\u001b[0m     color \u001b[39m=\u001b[39m ImageColor\u001b[39m.\u001b[39mgetcolor(color, mode)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 20\n",
    "loss_list = []\n",
    "\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    total_loss = []\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(data)\n",
    "\n",
    "        # Calculating loss\n",
    "        loss = loss_func(output, target)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Optimize the weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss.append(loss.item())\n",
    "    loss_list.append(sum(total_loss)/len(total_loss))\n",
    "    print('Training [{:.0f}%]\\tLoss: {:.4f}'.format(\n",
    "        100. * (epoch + 1) / epochs, loss_list[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_704/3555791888.py:19: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on test data:\n",
      "\tLoss: 1.4891\n",
      "\tAccuracy: 95.8%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        data = data\n",
    "        target = target\n",
    "        output = model(data)\n",
    "        pred = output.argmax(dim=1, keepdim=True) \n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        loss = loss_func(output, target)\n",
    "        total_loss.append(loss.item())\n",
    "    print('Performance on test data:\\n\\tLoss: {:.4f}\\n\\tAccuracy: {:.1f}%'.format(\n",
    "        sum(total_loss) / len(total_loss),\n",
    "        correct / len(test_loader) * 100 / BATCH_SIZE)\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8336230abddf85a168fff59aa340863a341785bf20719df20d9f0ccc1818ff6e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('qmind')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
